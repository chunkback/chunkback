![License: MIT](/assets/chunkback-app.svg)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue.svg)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-22+-green.svg)](https://nodejs.org/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](./CONTRIBUTING.md)
![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/4shub/chunkback/auto-version-publish.yml)
![NPM Version](https://img.shields.io/npm/v/chunkback?color=blue)

**ChunkBack lets you test your apps with LLM provider endpoints without having to pay for the providers.**

ChunkBack is a simple express server that emulates the response input and output of popular LLM providers, currently Gemini, Anthropic and OpenAI. Chunkback accepts a custom prompt language called [CBPL](./CBPL.md) that lets you customize the response to your applications.

## Quick Start

```
npx chunkback@latest
```

Then run in your terminal:

```
curl -X POST http://localhost:5654/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "swag",
    "messages": [
      {"role": "user", "content": "SAY \"First message\"\nCHUNKSIZE 3\nCHUNKLATENCY 50\nSAY \"Second message\""}
    ]
  }'
```

## Why use ChunkBack?

- **Deterministic API response** - You always will get back the same content you put in
- **Saves money** - When testing your application, you can stub out your LLM calls with this
- **Open Source** - You can see the code right there!
- **No extra services dependencies** - There's no DB no Redis no nothing, just server code

## Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md)

## License

MIT License - see [LICENSE](./LICENSE) for details.

## AI Notice

This project was built with the assistance of autogenerated coding libraries - most code was reviewed by a human (painstakingly). This README.md was handwritten, and should be kep that way.
